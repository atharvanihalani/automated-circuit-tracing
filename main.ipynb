{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Graph for Dallas Capital Query\n",
    "\n",
    "This notebook creates an attribution graph for the sentence:\n",
    "**\"Fact: The capital of the state containing Dallas is\"**\n",
    "\n",
    "We'll use the Gemma-2 (2B) model with GemmaScope transcoders to analyze the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Transcoders\n",
    "\n",
    "We'll load the Gemma-2-2B model with GemmaScope transcoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-2b with gemma transcoders...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbcd41d6e204d1f95f929795cb9de9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b2e3dcb4024d36bf63926fd2fc991d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"  # GemmaScope transcoders\n",
    "\n",
    "print(f\"Loading {model_name} with {transcoder_name} transcoders...\")\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Attribution Parameters\n",
    "\n",
    "Set up the parameters for attribution:\n",
    "- `prompt`: The input sentence to analyze\n",
    "- `max_n_logits`: Maximum number of output logits to attribute\n",
    "- `desired_logit_prob`: Cumulative probability threshold for logit selection\n",
    "- `max_feature_nodes`: Maximum number of feature nodes to include (lower = faster but less complete)\n",
    "- `batch_size`: Batch size for attribution computation\n",
    "- `offload`: Memory management strategy ('cpu', 'disk', or None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: The capital of the state containing Dallas is\n",
      "Max logits: 10\n",
      "Desired logit probability: 0.95\n",
      "Max feature nodes: 8192\n",
      "Batch size: 256\n",
      "Offload strategy: cpu\n"
     ]
    }
   ],
   "source": [
    "# Attribution parameters\n",
    "prompt = \"Fact: The capital of the state containing Dallas is\"\n",
    "max_n_logits = 10\n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192  # None for no limit, but will be slower\n",
    "batch_size = 256\n",
    "offload = 'cpu'  # Use 'disk' if running out of memory, None to keep everything on GPU\n",
    "verbose = True\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Max logits: {max_n_logits}\")\n",
    "print(f\"Desired logit probability: {desired_logit_prob}\")\n",
    "print(f\"Max feature nodes: {max_feature_nodes}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Offload strategy: {offload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Attribution\n",
    "\n",
    "This will compute the attribution graph showing the direct effects between features and output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attribution...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputation completed in 0.31s\n",
      "Found 9081 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.10s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7695\n",
      "Will include 8192 of 9081 feature nodes\n",
      "Input vectors built in 1.48s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.71s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:07<00:00, 1078.98it/s]\n",
      "Feature attributions completed in 7.60s\n",
      "Attribution completed in 15.62s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribution complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning attribution...\\n\")\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "print(\"\\nAttribution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Graph Statistics\n",
    "\n",
    "Let's examine the structure of the attribution graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,   127],\n",
       "        [    0,     1,   208],\n",
       "        [    0,     1,   355],\n",
       "        ...,\n",
       "        [   25,    10, 15131],\n",
       "        [   25,    10, 16302],\n",
       "        [   25,    10, 16326]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.active_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPH STATISTICS\n",
      "============================================================\n",
      "\n",
      "Input String: <bos>Fact: The capital of the state containing Dallas is\n",
      "Input Tokens: [2, 18143, 235292, 714, 6037, 576, 573, 2329, 10751, 26865, 603]\n",
      "Number of positions: 11\n",
      "\n",
      "Total active features: 9081\n",
      "Selected features for graph: 8192\n",
      "\n",
      "Graph Structure:\n",
      "  Feature nodes: 8192\n",
      "  Error nodes: 286 (26 layers × 11 positions)\n",
      "  Embedding nodes: 11\n",
      "  Logit nodes: 10\n",
      "  Total nodes: 8499\n",
      "\n",
      "Total non-zero edges: 19,022,999\n",
      "Adjacency matrix shape: torch.Size([8499, 8499])\n",
      "Adjacency matrix density: 26.34%\n",
      "\n",
      "Top 10 predicted logits:\n",
      "  1. ' Austin' (token 22605) - probability: 0.4453\n",
      "  2. ' not' (token 780) - probability: 0.0776\n",
      "  3. ' the' (token 573) - probability: 0.0532\n",
      "  4. ' Texas' (token 9447) - probability: 0.0415\n",
      "  5. ' Fort' (token 9778) - probability: 0.0366\n",
      "  6. ' Houston' (token 22898) - probability: 0.0286\n",
      "  7. ' Dallas' (token 26865) - probability: 0.0251\n",
      "  8. ' ' (token 235248) - probability: 0.0251\n",
      "  9. ' Oklahoma' (token 28239) - probability: 0.0197\n",
      "  10. ' San' (token 3250) - probability: 0.0153\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input information\n",
    "print(f\"\\nInput String: {graph.input_string}\")\n",
    "print(f\"Input Tokens: {graph.input_tokens.tolist()}\")\n",
    "print(f\"Number of positions: {graph.n_pos}\")\n",
    "\n",
    "# Feature information\n",
    "print(f\"\\nTotal active features: {len(graph.active_features)}\")\n",
    "print(f\"Selected features for graph: {len(graph.selected_features)}\")\n",
    "\n",
    "# Node structure\n",
    "n_layers = graph.cfg.n_layers\n",
    "n_pos = graph.n_pos\n",
    "n_error_nodes = n_layers * n_pos\n",
    "n_embed_nodes = n_pos\n",
    "n_logit_nodes = len(graph.logit_tokens)\n",
    "total_nodes = len(graph.selected_features) + n_error_nodes + n_embed_nodes + n_logit_nodes\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "print(f\"  Feature nodes: {len(graph.selected_features)}\")\n",
    "print(f\"  Error nodes: {n_error_nodes} ({n_layers} layers × {n_pos} positions)\")\n",
    "print(f\"  Embedding nodes: {n_embed_nodes}\")\n",
    "print(f\"  Logit nodes: {n_logit_nodes}\")\n",
    "print(f\"  Total nodes: {total_nodes}\")\n",
    "\n",
    "# Edge information\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "total_edges = (adjacency_matrix != 0).sum().item()\n",
    "print(f\"\\nTotal non-zero edges: {total_edges:,}\")\n",
    "print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n",
    "print(f\"Adjacency matrix density: {total_edges / (adjacency_matrix.shape[0] * adjacency_matrix.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "# Top logits\n",
    "print(f\"\\nTop {len(graph.logit_tokens)} predicted logits:\")\n",
    "for i, (token_id, prob) in enumerate(zip(graph.logit_tokens, graph.logit_probabilities)):\n",
    "    token_str = model.tokenizer.decode([token_id.item()])\n",
    "    print(f\"  {i+1}. '{token_str}' (token {token_id.item()}) - probability: {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Path Finding: Core DP Implementation\n",
    "\n",
    "Implement k-best paths algorithm using Dynamic Programming. This finds the exact top-K most influential paths by processing nodes in reverse topological order.\n",
    "\n",
    "**Optimization:** This version avoids expensive list copying by storing lightweight references during DP and reconstructing paths only at the end (~10-100x faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing topological order of the attribution graph...\n",
      "Topological order computed: 8499 nodes\n"
     ]
    }
   ],
   "source": [
    "def compute_topological_order(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute topological order using Kahn's algorithm.\n",
    "    \n",
    "    Args:\n",
    "        adjacency_matrix: torch.Tensor of shape (n_nodes, n_nodes)\n",
    "                         where adjacency_matrix[target, source] represents edge from source -> target\n",
    "    \n",
    "    Returns:\n",
    "        list: Topological order of node indices\n",
    "    \"\"\"\n",
    "    n_nodes = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Compute in-degrees: for each node, count how many edges point TO it\n",
    "    in_degree = (adjacency_matrix != 0).sum(dim=1).cpu()\n",
    "    \n",
    "    # Initialize queue with nodes that have no incoming edges\n",
    "    queue = [i for i in range(n_nodes) if in_degree[i] == 0]\n",
    "    topo_order = []\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        topo_order.append(node)\n",
    "        \n",
    "        # For each outgoing edge from this node\n",
    "        outgoing_edges = (adjacency_matrix[:, node] != 0).cpu()\n",
    "        \n",
    "        for target in range(n_nodes):\n",
    "            if outgoing_edges[target]:\n",
    "                in_degree[target] -= 1\n",
    "                if in_degree[target] == 0:\n",
    "                    queue.append(target)\n",
    "    \n",
    "    if len(topo_order) != n_nodes:\n",
    "        print(f\"Warning: Graph contains a cycle! Only {len(topo_order)}/{n_nodes} nodes ordered.\")\n",
    "        remaining = [i for i in range(n_nodes) if i not in topo_order]\n",
    "        topo_order.extend(remaining)\n",
    "    \n",
    "    return topo_order\n",
    "\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "\n",
    "print(\"Computing topological order of the attribution graph...\")\n",
    "topo_order = compute_topological_order(adjacency_matrix)\n",
    "print(f\"Topological order computed: {len(topo_order)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Find Complete Paths: Embedding → Austin Logit\n",
    "\n",
    "Find the top-10 most influential complete paths from input embedding tokens to the Austin logit prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path finding functions loaded (optimized version)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from circuit_tracer.graph import compute_node_influence\n",
    "\n",
    "@dataclass\n",
    "class Path:\n",
    "    \"\"\"Represents a path through the attribution graph.\"\"\"\n",
    "    nodes: List[int]\n",
    "    edges: List[float]\n",
    "    score: float\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    \n",
    "    def get_node_types(self, graph) -> List[str]:\n",
    "        \"\"\"Return node types: 'feature', 'error', 'embed', 'logit'.\"\"\"\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        types = []\n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                types.append('feature')\n",
    "            elif node < n_features + n_errors:\n",
    "                types.append('error')\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                types.append('embed')\n",
    "            else:\n",
    "                types.append('logit')\n",
    "        return types\n",
    "    \n",
    "    def get_node_descriptions(self, graph, tokenizer) -> List[str]:\n",
    "        \"\"\"Return human-readable descriptions for each node.\"\"\"\n",
    "        descriptions = []\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                layer, pos, feat_idx = graph.active_features[graph.selected_features[node]].tolist()\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Feature L{layer}:F{feat_idx} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors:\n",
    "                error_idx = node - n_features\n",
    "                layer = error_idx // graph.n_pos\n",
    "                pos = error_idx % graph.n_pos\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Error L{layer} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                pos = node - n_features - n_errors\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Embedding @ pos {pos} ('{token}')\")\n",
    "            else:\n",
    "                logit_idx = node - n_features - n_errors - n_embeds\n",
    "                token = tokenizer.decode([graph.logit_tokens[logit_idx]])\n",
    "                prob = graph.logit_probabilities[logit_idx].item()\n",
    "                descriptions.append(f\"Logit '{token}' (p={prob:.4f})\")\n",
    "        \n",
    "        return descriptions\n",
    "\n",
    "\n",
    "def find_k_best_paths_dp(adj_matrix, source_nodes, sink_node, topo_order, k=10, verbose=True):\n",
    "    \"\"\"OPTIMIZED: Find top-K paths using DP. Stores lightweight references, reconstructs at end.\"\"\"\n",
    "    best_path_refs = {}\n",
    "    best_path_refs[sink_node] = [(None, None, 1.0)]\n",
    "    \n",
    "    iterator = reversed(topo_order) if not verbose else tqdm(\n",
    "        reversed(topo_order), desc=\"DP path finding (optimized)\", total=len(topo_order)\n",
    "    )\n",
    "    \n",
    "    for node in iterator:\n",
    "        if node == sink_node:\n",
    "            continue\n",
    "        \n",
    "        outgoing_weights = adj_matrix[:, node]\n",
    "        successors = torch.where(outgoing_weights != 0)[0]\n",
    "        \n",
    "        if len(successors) == 0:\n",
    "            best_path_refs[node] = []\n",
    "            continue\n",
    "        \n",
    "        candidate_refs = []\n",
    "        for succ in successors:\n",
    "            succ_idx = succ.item()\n",
    "            if succ_idx not in best_path_refs or len(best_path_refs[succ_idx]) == 0:\n",
    "                continue\n",
    "            \n",
    "            edge_weight = outgoing_weights[succ].item()\n",
    "            for succ_next, succ_edge, path_score in best_path_refs[succ_idx]:\n",
    "                new_score = abs(edge_weight) * path_score\n",
    "                candidate_refs.append((succ_idx, edge_weight, new_score))\n",
    "        \n",
    "        candidate_refs.sort(key=lambda x: x[2], reverse=True)\n",
    "        best_path_refs[node] = candidate_refs[:k]\n",
    "    \n",
    "    def reconstruct_path(start_node, path_ref_index):\n",
    "        \"\"\"Reconstruct full path by following successor chain.\"\"\"\n",
    "        nodes, edges = [start_node], []\n",
    "        current_node, current_ref_idx = start_node, path_ref_index\n",
    "        \n",
    "        while True:\n",
    "            next_node, edge_weight, score = best_path_refs[current_node][current_ref_idx]\n",
    "            if next_node is None:\n",
    "                break\n",
    "            \n",
    "            nodes.append(next_node)\n",
    "            edges.append(edge_weight)\n",
    "            current_node = next_node\n",
    "            \n",
    "            target_score = score / abs(edge_weight)\n",
    "            current_ref_idx = 0\n",
    "            for i, (nn, ne, ns) in enumerate(best_path_refs[current_node]):\n",
    "                if abs(ns - target_score) < 1e-9:\n",
    "                    current_ref_idx = i\n",
    "                    break\n",
    "        \n",
    "        return Path(nodes=nodes, edges=edges, score=best_path_refs[start_node][path_ref_index][2])\n",
    "    \n",
    "    all_source_paths = []\n",
    "    for source in source_nodes:\n",
    "        if source in best_path_refs and len(best_path_refs[source]) > 0:\n",
    "            for path_idx in range(len(best_path_refs[source])):\n",
    "                all_source_paths.append(reconstruct_path(source, path_idx))\n",
    "    \n",
    "    all_source_paths.sort(key=lambda p: p.score, reverse=True)\n",
    "    return all_source_paths[:k]\n",
    "\n",
    "\n",
    "print(\"✅ Path finding functions loaded (optimized version)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top-10 complete paths: Embeddings [8478:8489] → Austin [8489]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DP path finding (optimized): 100%|██████████| 8499/8499 [03:10<00:00, 44.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 10 complete paths!\n",
      "\n",
      "Path #1 (Score: 486778389695780618240.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #2 (Score: 390916144627167264768.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #3 (Score: 363696143468630310912.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 4 (' capital')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #4 (Score: 338321993924307255296.00000000, Length: 26)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #5 (Score: 301418302882676801536.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #6 (Score: 292210203932084797440.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 4 (' capital')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #7 (Score: 292072732130488057856.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 4 (' capital')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #8 (Score: 279884119724888883200.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #9 (Score: 278784621023661228032.00000000, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #10 (Score: 271695564772547002368.00000000, Length: 26)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define node indices\n",
    "n_features = len(graph.selected_features)\n",
    "n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "n_embeds = graph.n_pos\n",
    "n_logits = len(graph.logit_tokens)\n",
    "\n",
    "embed_start = n_features + n_errors\n",
    "embed_end = embed_start + n_embeds\n",
    "embed_nodes = list(range(embed_start, embed_end))\n",
    "austin_logit = embed_end  # Index 8489\n",
    "\n",
    "print(f\"Finding top-10 complete paths: Embeddings [{embed_start}:{embed_end}] → Austin [{austin_logit}]\")\n",
    "print()\n",
    "\n",
    "# Find complete paths\n",
    "complete_paths = find_k_best_paths_dp(\n",
    "    adj_matrix=adjacency_matrix,\n",
    "    source_nodes=embed_nodes,\n",
    "    sink_node=austin_logit,\n",
    "    topo_order=topo_order,\n",
    "    k=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Found {len(complete_paths)} complete paths!\")\n",
    "print()\n",
    "\n",
    "# Display paths\n",
    "for rank, path in enumerate(complete_paths, 1):\n",
    "    node_descs = path.get_node_descriptions(graph, model.tokenizer)\n",
    "    node_types = path.get_node_types(graph)\n",
    "    \n",
    "    print(f\"Path #{rank} (Score: {path.score:.8f}, Length: {len(path)})\")\n",
    "    print(f\"  {' → '.join(node_types)}\")\n",
    "    print(f\"  Start: {node_descs[0]}\")\n",
    "    print(f\"  End: {node_descs[-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Export Paths to DataFrame\n",
    "\n",
    "Create a structured DataFrame with path details for analysis and export to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Paths DataFrame:\n",
      "   rank  influence_score  length  \\\n",
      "0     1     4.867784e+20      27   \n",
      "1     2     3.909161e+20      27   \n",
      "2     3     3.636961e+20      27   \n",
      "3     4     3.383220e+20      26   \n",
      "4     5     3.014183e+20      27   \n",
      "5     6     2.922102e+20      27   \n",
      "6     7     2.920727e+20      27   \n",
      "7     8     2.798841e+20      27   \n",
      "8     9     2.787846e+20      27   \n",
      "9    10     2.716956e+20      26   \n",
      "\n",
      "                                        path_summary  \n",
      "0  embed → feature → feature → feature → feature ...  \n",
      "1  embed → feature → feature → feature → feature ...  \n",
      "2  embed → feature → feature → feature → feature ...  \n",
      "3  embed → feature → feature → feature → feature ...  \n",
      "4  embed → feature → feature → feature → feature ...  \n",
      "5  embed → feature → feature → feature → feature ...  \n",
      "6  embed → feature → feature → feature → feature ...  \n",
      "7  embed → feature → feature → feature → feature ...  \n",
      "8  embed → feature → feature → feature → feature ...  \n",
      "9  embed → feature → feature → feature → feature ...  \n",
      "\n",
      "✅ Saved to complete_paths_austin.csv\n"
     ]
    }
   ],
   "source": [
    "def paths_to_dataframe(paths, graph, tokenizer):\n",
    "    \"\"\"Convert list of Path objects to pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for rank, path in enumerate(paths, 1):\n",
    "        node_types = path.get_node_types(graph)\n",
    "        node_descs = path.get_node_descriptions(graph, tokenizer)\n",
    "        \n",
    "        rows.append({\n",
    "            'rank': rank,\n",
    "            'influence_score': path.score,\n",
    "            'length': len(path),\n",
    "            'start_type': node_types[0],\n",
    "            'end_type': node_types[-1],\n",
    "            'start_description': node_descs[0],\n",
    "            'end_description': node_descs[-1],\n",
    "            'path_summary': ' → '.join(node_types),\n",
    "            'full_path': ' → '.join(node_descs),\n",
    "            'min_edge_weight': min(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'max_edge_weight': max(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'avg_edge_weight': sum(abs(w) for w in path.edges) / len(path.edges) if path.edges else 0.0,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_complete = paths_to_dataframe(complete_paths, graph, model.tokenizer)\n",
    "\n",
    "print(\"Complete Paths DataFrame:\")\n",
    "print(df_complete[['rank', 'influence_score', 'length', 'path_summary']])\n",
    "print()\n",
    "\n",
    "# Save to CSV\n",
    "df_complete.to_csv('complete_paths_austin.csv', index=False)\n",
    "print(\"✅ Saved to complete_paths_austin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Graph\n",
    "\n",
    "Save the attribution graph to a .pt file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save graph\n",
    "graph_dir = Path('graphs')\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_name = 'dallas_capital_attribution.pt'\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "print(f\"Saving graph to {graph_path}...\")\n",
    "graph.to_pt(graph_path)\n",
    "print(f\"Graph saved successfully! (Size: {graph_path.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Visualization Files\n",
    "\n",
    "Generate graph files for interactive visualization. The pruning thresholds control how much of the graph to keep:\n",
    "- `node_threshold`: Keep minimum nodes whose cumulative influence >= this value\n",
    "- `edge_threshold`: Keep minimum edges whose cumulative influence >= this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-capital\"  # Name for this graph\n",
    "graph_file_dir = './graph_files'\n",
    "node_threshold = 0.8  # Keep nodes explaining 80% of influence\n",
    "edge_threshold = 0.98  # Keep edges explaining 98% of influence\n",
    "\n",
    "print(f\"Creating visualization files with slug '{slug}'...\")\n",
    "print(f\"Node threshold: {node_threshold}, Edge threshold: {edge_threshold}\")\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")\n",
    "\n",
    "print(f\"Visualization files created in {graph_file_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch Visualization Server\n",
    "\n",
    "Start a local server to interactively explore the attribution graph.\n",
    "\n",
    "**Features:**\n",
    "- Click to select nodes\n",
    "- Ctrl/Cmd+Click to pin/unpin nodes to your subgraph\n",
    "- G+Click on nodes to group them into supernodes\n",
    "- Edit node descriptions by clicking the edit button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "from IPython.display import IFrame\n",
    "\n",
    "port = 8046\n",
    "print(f\"Starting visualization server on port {port}...\")\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "print(f\"\\nVisualization server is running!\")\n",
    "print(f\"Open your graph here: http://localhost:{port}/index.html\")\n",
    "print(f\"\\nTo stop the server later, run: server.stop()\")\n",
    "\n",
    "# Display in iframe\n",
    "display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stop the Server (Optional)\n",
    "\n",
    "Uncomment and run this cell when you're done with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server.stop()\n",
    "# print(\"Server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neel-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
