{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Graph for Dallas Capital Query\n",
    "\n",
    "This notebook creates an attribution graph for the sentence:\n",
    "**\"Fact: The capital of the state containing Dallas is\"**\n",
    "\n",
    "We'll use the Gemma-2 (2B) model with GemmaScope transcoders to analyze the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Transcoders\n",
    "\n",
    "We'll load the Gemma-2-2B model with GemmaScope transcoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-2b with gemma transcoders...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ffb257501c408eb21af578bb6608cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636bdbacdaa648389c7a12e6218c02ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d712c59c352b4b48aebe897f64c6bb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_0/width_16k/average_l0_76/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de638880754bc4b4946b4297f6ad60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_5/width_16k/average_l0_87/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce56b060294f4cb0d02be99ae067df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_3/width_16k/average_l0_54/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13eb300d31a43ffad6fec038d82467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_7/width_16k/average_l0_70/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab4f7b3f61a4c3987ba95085a9775b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_4/width_16k/average_l0_88/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2ef77af47b47f9921b59fc54faed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_6/width_16k/average_l0_95/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2241424e93de4b58ba6854ac196ded96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_2/width_16k/average_l0_49/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edff990aaba4429197c7a371a87bc953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_1/width_16k/average_l0_65/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3d8dc8f08d43dcb591296adf07bd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_8/width_16k/average_l0_52/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec226bb140043b7bb03d263d4c28995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_9/width_16k/average_l0_72/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc05869a64a0474aa970ca3f4ccd261a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_12/width_16k/average_l0_6/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b53113833654dfaa21aecebe6d9b541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_11/width_16k/average_l0_5/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2630844c14da4035930d39533ebbdc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_10/width_16k/average_l0_88/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707f5dc6709c4c89a19ad7d4c06666fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_14/width_16k/average_l0_8/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698cadab2860410a8d7fc86262339cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_13/width_16k/average_l0_8/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b63e7d0bec41fabe2cd8c58f7dfa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_15/width_16k/average_l0_8/params.n(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee7a2accb9c43188815c6b72399a193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_16/width_16k/average_l0_10/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515260bf9e934d62bb210702776e042c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_17/width_16k/average_l0_12/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f552c82de7dd46ba88fa7b33f34383e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_20/width_16k/average_l0_11/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3123adf7b33a4986930a7f06d54019be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_19/width_16k/average_l0_12/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3263fc06a84993874a3da757c22e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_21/width_16k/average_l0_13/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61635cc729943f7be3c65ebf4ff6702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_18/width_16k/average_l0_13/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab3d0c88e604cd38a2b394dffdb9286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_22/width_16k/average_l0_15/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea51352243374a82b7d500f21200d89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_23/width_16k/average_l0_25/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46c2709ca38471baf1f6e6a8a34fb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_24/width_16k/average_l0_37/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1b1ba340f349f2849aec13d500f824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_25/width_16k/average_l0_41/params.(…):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4ccb1230f549e58ec930dd8bc4c1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058a17c0a61a40bda3ed58dc45a30693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad07483184949d889e40894d30f716c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9626c30683e4b9cb556645e33cd67a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd630bfd4c204824a3fe45ed249c09b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b5fa0d15c4df49e651490fad0d1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d883155d79ac4153a4fc91ace7bbbcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad9ea6d10704aba9f03ba667a6b9446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f115e0a1d514e06bb12756a73d3ccbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14f44882813409aae1d5034b01eaacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75dbb0037eb4e768fe5cb5a0689546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2962db1cead4cc49ee2c2f4562bdb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"  # GemmaScope transcoders\n",
    "\n",
    "print(f\"Loading {model_name} with {transcoder_name} transcoders...\")\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Attribution Parameters\n",
    "\n",
    "Set up the parameters for attribution:\n",
    "- `prompt`: The input sentence to analyze\n",
    "- `max_n_logits`: Maximum number of output logits to attribute\n",
    "- `desired_logit_prob`: Cumulative probability threshold for logit selection\n",
    "- `max_feature_nodes`: Maximum number of feature nodes to include (lower = faster but less complete)\n",
    "- `batch_size`: Batch size for attribution computation\n",
    "- `offload`: Memory management strategy ('cpu', 'disk', or None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: The capital of the state containing Dallas is\n",
      "Max logits: 10\n",
      "Desired logit probability: 0.95\n",
      "Max feature nodes: 8192\n",
      "Batch size: 256\n",
      "Offload strategy: cpu\n"
     ]
    }
   ],
   "source": [
    "# Attribution parameters\n",
    "prompt = \"Fact: The capital of the state containing Dallas is\"\n",
    "max_n_logits = 10\n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192  # None for no limit, but will be slower\n",
    "batch_size = 256\n",
    "offload = 'cpu'  # Use 'disk' if running out of memory, None to keep everything on GPU\n",
    "verbose = True\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Max logits: {max_n_logits}\")\n",
    "print(f\"Desired logit probability: {desired_logit_prob}\")\n",
    "print(f\"Max feature nodes: {max_feature_nodes}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Offload strategy: {offload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Attribution\n",
    "\n",
    "This will compute the attribution graph showing the direct effects between features and output logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attribution...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputation completed in 0.25s\n",
      "Found 9081 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.08s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7695\n",
      "Will include 8192 of 9081 feature nodes\n",
      "Input vectors built in 1.24s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.29s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:07<00:00, 1076.95it/s]\n",
      "Feature attributions completed in 7.70s\n",
      "Attribution completed in 13.92s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribution complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning attribution...\\n\")\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "print(\"\\nAttribution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Graph Statistics\n",
    "\n",
    "Let's examine the structure of the attribution graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of active features: 9081\n",
      "length of adjacency matrix: 8499\n",
      "number of \"activation values\": 9081\n"
     ]
    }
   ],
   "source": [
    "print(f'number of active features: {len(graph.active_features)}')\n",
    "print(f'length of adjacency matrix: {len(graph.adjacency_matrix)}')\n",
    "print(f'number of \"activation values\": {len(graph.activation_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,   127],\n",
       "        [    0,     1,   208],\n",
       "        [    0,     1,   355],\n",
       "        ...,\n",
       "        [   25,    10, 15131],\n",
       "        [   25,    10, 16302],\n",
       "        [   25,    10, 16326]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.active_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPH STATISTICS\n",
      "============================================================\n",
      "\n",
      "Input String: <bos>Fact: The capital of the state containing Dallas is\n",
      "Input Tokens: [2, 18143, 235292, 714, 6037, 576, 573, 2329, 10751, 26865, 603]\n",
      "Number of positions: 11\n",
      "\n",
      "Total active features: 9081\n",
      "Selected features for graph: 8192\n",
      "\n",
      "Graph Structure:\n",
      "  Feature nodes: 8192\n",
      "  Error nodes: 286 (26 layers × 11 positions)\n",
      "  Embedding nodes: 11\n",
      "  Logit nodes: 10\n",
      "  Total nodes: 8499\n",
      "\n",
      "Total non-zero edges: 19,022,999\n",
      "Adjacency matrix shape: torch.Size([8499, 8499])\n",
      "Adjacency matrix density: 26.34%\n",
      "\n",
      "Top 10 predicted logits:\n",
      "  1. ' Austin' (token 22605) - probability: 0.4453\n",
      "  2. ' not' (token 780) - probability: 0.0776\n",
      "  3. ' the' (token 573) - probability: 0.0532\n",
      "  4. ' Texas' (token 9447) - probability: 0.0415\n",
      "  5. ' Fort' (token 9778) - probability: 0.0366\n",
      "  6. ' Houston' (token 22898) - probability: 0.0286\n",
      "  7. ' Dallas' (token 26865) - probability: 0.0251\n",
      "  8. ' ' (token 235248) - probability: 0.0251\n",
      "  9. ' Oklahoma' (token 28239) - probability: 0.0197\n",
      "  10. ' San' (token 3250) - probability: 0.0153\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input information\n",
    "print(f\"\\nInput String: {graph.input_string}\")\n",
    "print(f\"Input Tokens: {graph.input_tokens.tolist()}\")\n",
    "print(f\"Number of positions: {graph.n_pos}\")\n",
    "\n",
    "# Feature information\n",
    "print(f\"\\nTotal active features: {len(graph.active_features)}\")\n",
    "print(f\"Selected features for graph: {len(graph.selected_features)}\")\n",
    "\n",
    "# Node structure\n",
    "n_layers = graph.cfg.n_layers\n",
    "n_pos = graph.n_pos\n",
    "n_error_nodes = n_layers * n_pos\n",
    "n_embed_nodes = n_pos\n",
    "n_logit_nodes = len(graph.logit_tokens)\n",
    "total_nodes = len(graph.selected_features) + n_error_nodes + n_embed_nodes + n_logit_nodes\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "print(f\"  Feature nodes: {len(graph.selected_features)}\")\n",
    "print(f\"  Error nodes: {n_error_nodes} ({n_layers} layers × {n_pos} positions)\")\n",
    "print(f\"  Embedding nodes: {n_embed_nodes}\")\n",
    "print(f\"  Logit nodes: {n_logit_nodes}\")\n",
    "print(f\"  Total nodes: {total_nodes}\")\n",
    "\n",
    "# Edge information\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "total_edges = (adjacency_matrix != 0).sum().item()\n",
    "print(f\"\\nTotal non-zero edges: {total_edges:,}\")\n",
    "print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n",
    "print(f\"Adjacency matrix density: {total_edges / (adjacency_matrix.shape[0] * adjacency_matrix.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "# Top logits\n",
    "print(f\"\\nTop {len(graph.logit_tokens)} predicted logits:\")\n",
    "for i, (token_id, prob) in enumerate(zip(graph.logit_tokens, graph.logit_probabilities)):\n",
    "    token_str = model.tokenizer.decode([token_id.item()])\n",
    "    print(f\"  {i+1}. '{token_str}' (token {token_id.item()}) - probability: {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Path Finding: Core DP Implementation\n",
    "\n",
    "Implement k-best paths algorithm using Dynamic Programming. This finds the exact top-K most influential paths by processing nodes in reverse topological order.\n",
    "\n",
    "**Optimization:** This version avoids expensive list copying by storing lightweight references during DP and reconstructing paths only at the end (~10-100x faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing topological order of the attribution graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological order computed: 8499 nodes\n"
     ]
    }
   ],
   "source": [
    "def compute_topological_order(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute topological order using Kahn's algorithm.\n",
    "    \n",
    "    Args:\n",
    "        adjacency_matrix: torch.Tensor of shape (n_nodes, n_nodes)\n",
    "                         where adjacency_matrix[target, source] represents edge from source -> target\n",
    "    \n",
    "    Returns:\n",
    "        list: Topological order of node indices\n",
    "    \"\"\"\n",
    "    n_nodes = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Compute in-degrees: for each node, count how many edges point TO it\n",
    "    in_degree = (adjacency_matrix != 0).sum(dim=1).cpu()\n",
    "    \n",
    "    # Initialize queue with nodes that have no incoming edges\n",
    "    queue = [i for i in range(n_nodes) if in_degree[i] == 0]\n",
    "    topo_order = []\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        topo_order.append(node)\n",
    "        \n",
    "        # For each outgoing edge from this node\n",
    "        outgoing_edges = (adjacency_matrix[:, node] != 0).cpu()\n",
    "        \n",
    "        for target in range(n_nodes):\n",
    "            if outgoing_edges[target]:\n",
    "                in_degree[target] -= 1\n",
    "                if in_degree[target] == 0:\n",
    "                    queue.append(target)\n",
    "    \n",
    "    if len(topo_order) != n_nodes:\n",
    "        print(f\"Warning: Graph contains a cycle! Only {len(topo_order)}/{n_nodes} nodes ordered.\")\n",
    "        remaining = [i for i in range(n_nodes) if i not in topo_order]\n",
    "        topo_order.extend(remaining)\n",
    "    \n",
    "    return topo_order\n",
    "\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "\n",
    "print(\"Computing topological order of the attribution graph...\")\n",
    "topo_order = compute_topological_order(adjacency_matrix)\n",
    "print(f\"Topological order computed: {len(topo_order)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topo_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Find Complete Paths: Embedding → Austin Logit\n",
    "\n",
    "Find the top-10 most influential complete paths from input embedding tokens to the Austin logit prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path finding functions loaded (optimized version)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from circuit_tracer.graph import compute_node_influence\n",
    "\n",
    "@dataclass\n",
    "class Path:\n",
    "    \"\"\"Represents a path through the attribution graph.\"\"\"\n",
    "    nodes: List[int]\n",
    "    edges: List[float]\n",
    "    score: float\n",
    "    # averaged_score: float\n",
    "    final_score: float\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    \n",
    "    def get_node_types(self, graph) -> List[str]:\n",
    "        \"\"\"Return node types: 'feature', 'error', 'embed', 'logit'.\"\"\"\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        types = []\n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                types.append('feature')\n",
    "            elif node < n_features + n_errors:\n",
    "                types.append('error')\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                types.append('embed')\n",
    "            else:\n",
    "                types.append('logit')\n",
    "        return types\n",
    "    \n",
    "    def get_node_descriptions(self, graph, tokenizer) -> List[str]:\n",
    "        \"\"\"Return human-readable descriptions for each node.\"\"\"\n",
    "        descriptions = []\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                layer, pos, feat_idx = graph.active_features[graph.selected_features[node]].tolist()\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Feature L{layer}:F{feat_idx} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors:\n",
    "                error_idx = node - n_features\n",
    "                layer = error_idx // graph.n_pos\n",
    "                pos = error_idx % graph.n_pos\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Error L{layer} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                pos = node - n_features - n_errors\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Embedding @ pos {pos} ('{token}')\")\n",
    "            else:\n",
    "                logit_idx = node - n_features - n_errors - n_embeds\n",
    "                token = tokenizer.decode([graph.logit_tokens[logit_idx]])\n",
    "                prob = graph.logit_probabilities[logit_idx].item()\n",
    "                descriptions.append(f\"Logit '{token}' (p={prob:.4f})\")\n",
    "        \n",
    "        return descriptions\n",
    "\n",
    "\n",
    "def find_k_best_paths_dp(adj_matrix, source_nodes, sink_node, topo_order, k=10, verbose=True):\n",
    "    \"\"\"OPTIMIZED: Find top-K paths using DP. Stores lightweight references, reconstructs at end.\"\"\"\n",
    "    best_path_refs = {}\n",
    "    best_path_refs[sink_node] = [(None, None, 1.0)]\n",
    "    \n",
    "    iterator = reversed(topo_order) if not verbose else tqdm(\n",
    "        reversed(topo_order), desc=\"DP path finding (optimized)\", total=len(topo_order)\n",
    "    )\n",
    "    \n",
    "    for node in iterator:\n",
    "        if node == sink_node:\n",
    "            continue\n",
    "        \n",
    "        outgoing_weights = adj_matrix[:, node]\n",
    "        successors = torch.where(outgoing_weights != 0)[0]\n",
    "        \n",
    "        if len(successors) == 0:\n",
    "            best_path_refs[node] = []\n",
    "            continue\n",
    "        \n",
    "        candidate_refs = []\n",
    "        for succ in successors:\n",
    "            succ_idx = succ.item()\n",
    "            if succ_idx not in best_path_refs or len(best_path_refs[succ_idx]) == 0:\n",
    "                continue\n",
    "            \n",
    "            edge_weight = outgoing_weights[succ].item()\n",
    "            for succ_next, succ_edge, path_score in best_path_refs[succ_idx]:\n",
    "                new_score = abs(edge_weight) * path_score\n",
    "                candidate_refs.append((succ_idx, edge_weight, new_score))\n",
    "        \n",
    "        candidate_refs.sort(key=lambda x: x[2], reverse=True)\n",
    "        best_path_refs[node] = candidate_refs[:k]\n",
    "    \n",
    "    def reconstruct_path(start_node, path_ref_index):\n",
    "        \"\"\"Reconstruct full path by following successor chain.\"\"\"\n",
    "        nodes, edges = [start_node], []\n",
    "        current_node, current_ref_idx = start_node, path_ref_index\n",
    "        \n",
    "        while True:\n",
    "            next_node, edge_weight, score = best_path_refs[current_node][current_ref_idx]\n",
    "            if next_node is None:\n",
    "                break\n",
    "            \n",
    "            nodes.append(next_node)\n",
    "            edges.append(edge_weight)\n",
    "            current_node = next_node\n",
    "            \n",
    "            target_score = score / abs(edge_weight)\n",
    "            current_ref_idx = 0\n",
    "            for i, (nn, ne, ns) in enumerate(best_path_refs[current_node]):\n",
    "                if abs(ns - target_score) < 1e-9:\n",
    "                    current_ref_idx = i\n",
    "                    break\n",
    "        \n",
    "        return Path(nodes=nodes, edges=edges, score=best_path_refs[start_node][path_ref_index][2], final_score=0.0)\n",
    "    \n",
    "    all_source_paths = []\n",
    "    for source in source_nodes:\n",
    "        if source in best_path_refs and len(best_path_refs[source]) > 0:\n",
    "            for path_idx in range(len(best_path_refs[source])):\n",
    "                all_source_paths.append(reconstruct_path(source, path_idx))\n",
    "\n",
    "    return all_source_paths\n",
    "    \n",
    "    # for path in all_source_paths:\n",
    "    #     path.averaged_score = path.score / len(path.edges)\n",
    "    #     path.final_score = sum(path.edges) / len(path.edges)\n",
    "    \n",
    "    # all_source_paths.sort(key=lambda p: p.final_score, reverse=True)\n",
    "    # return all_source_paths[:k]\n",
    "\n",
    "\n",
    "print(\"✅ Path finding functions loaded (optimized version)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8499"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 9078, 9079, 9080])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_details(matrix_idx: int) -> tuple[int, int, int]:\n",
    "    assert matrix_idx < len(graph.selected_features), 'This node is not an active feature'\n",
    "    feature_idx = graph.selected_features[matrix_idx]\n",
    "    layer, token_pos, attribution_idx = graph.active_features[feature_idx]\n",
    "\n",
    "    return (layer.item(), token_pos.item(), attribution_idx.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_idx_to_explanation(matrix_idx: int):\n",
    "    layer, __, feature_idx = get_node_details(matrix_idx)\n",
    "\n",
    "    url = f'https://www.neuronpedia.org/gemma-2-2b/{layer}-gemmascope-transcoder-16k/{feature_idx}'\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "    body = soup.find('html').find('body')\n",
    "    idx_a = str(body).find('explanationModelName')\n",
    "    target_substring_large = str(body)[idx_a-200:idx_a]\n",
    "    assert 'description' in target_substring_large\n",
    "\n",
    "    idx_b = target_substring_large.find('description')\n",
    "    const_1 = 16\n",
    "    const_2 = 5\n",
    "    target_substring_final = target_substring_large[idx_b + const_1: -const_2]\n",
    "\n",
    "    return target_substring_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node indices\n",
    "n_features = len(graph.selected_features)\n",
    "n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "n_embeds = graph.n_pos\n",
    "n_logits = len(graph.logit_tokens)\n",
    "\n",
    "embed_start = n_features + n_errors\n",
    "embed_end = embed_start + n_embeds\n",
    "embed_nodes = list(range(embed_start, embed_end))\n",
    "austin_logit = embed_end  # Index 8489\n",
    "\n",
    "print(f\"Finding top-10 complete paths: Embeddings [{embed_start}:{embed_end}] → Austin [{austin_logit}]\")\n",
    "print()\n",
    "\n",
    "# Find complete paths\n",
    "all_complete_paths = find_k_best_paths_dp(\n",
    "    adj_matrix=adjacency_matrix,\n",
    "    source_nodes=embed_nodes,\n",
    "    sink_node=austin_logit,\n",
    "    topo_order=topo_order,\n",
    "    k=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_complete_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path(nodes=[8478, 1110, 1575, 1992, 2543, 3286, 4254, 5075, 5687, 6142, 6691, 7251, 7338, 7415, 7478, 7541, 7627, 7702, 7783, 7858, 7931, 7996, 8051, 8152, 8186, 8489], edges=[15.9375, 1.21875, -2.578125, -9.375, -12.875, -2.96875, -5.9375, -23.875, -15.5, -12.875, -7.8125, -11.4375, 0.392578125, 1.6875, 24.0, 10.9375, 18.75, 20.625, 19.625, 21.125, 19.75, 16.375, 0.01129150390625, -20.25, -1.921875], score=1.7973355927228822e+20, final_score=1.72115478515625)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complete_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This node is not an active feature\n",
      " words or phrases that appear in legal or technical documents, like names of laws, legal terms (pleaded, testified), and technical terms, especially when abbreviated or in code\n",
      " code snippets related to licensing, copyrights, or importing libraries\n",
      " code snippets and documentation references, possibly related to web development\n",
      "code snippets and license agreements\n",
      " law related terminology and references to specific cases or legal entities.\n",
      " code and file paths\n",
      " a variety of reference codes, abbreviations, and identifiers from different fields.\n",
      " code snippets with specific coding keywords and markup tags\n",
      " various code snippets\n",
      " content related to scientific publications or medical procedures, possibly extracting patient data from research papers\n",
      "scientific or technical words and jargon\n",
      " technical writing related to legal, medical, or engineering contexts\n",
      " references to geographic locations, especially cities and regions.\n",
      "the word \\\\\\\"capital\\\\\\\" and sometimes letters\n",
      "capital\n",
      "capital\n",
      " the word \\\\\\\"capital\\\\\\\" and words that often accompany it in business contexts\n",
      " the word \\\\\\\"capital\\\\\\\"\n",
      "the word \\\\\\\"capital\\\\\\\" in economic or legal contexts\n",
      " the word \\\\\\\"capital\\\\\\\" in the context of law, finance, and geography\n",
      " the word \\\\\\\"capital\\\\\\\" followed by words related to assets and finance.\n",
      " words related to finance and capital\n",
      " locations and legal case identifiers\n",
      " place names\n",
      "This node is not an active feature\n"
     ]
    }
   ],
   "source": [
    "mypath = all_complete_paths[0]\n",
    "# print(f'len graph active featrues: {len(graph.active_features)}')\n",
    "\n",
    "for node in mypath.nodes:\n",
    "    # print(f'node: {node}')\n",
    "    try:\n",
    "        node_explanation = matrix_idx_to_explanation(node)\n",
    "        print(node_explanation)\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 10 complete paths!\n",
      "\n",
      "Path #1 (Score: 3.79435847, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #2 (Score: 2.34305749, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #3 (Score: 2.34305749, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #4 (Score: 2.34305749, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #5 (Score: 2.34305749, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #6 (Score: 2.32217407, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #7 (Score: 2.28990479, Length: 26)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #8 (Score: 2.21340003, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #9 (Score: 2.18230027, Length: 27)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n",
      "Path #10 (Score: 2.12271729, Length: 26)\n",
      "  embed → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → feature → logit\n",
      "  Start: Embedding @ pos 2 (':')\n",
      "  End: Logit ' Austin' (p=0.4453)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in all_complete_paths:\n",
    "    path.final_score = sum(path.edges) / len(path.edges)\n",
    "\n",
    "all_complete_paths_sorted = sorted(all_complete_paths, key=lambda p: p.final_score, reverse=True)\n",
    "top_k_complete_paths = all_complete_paths_sorted[:10]\n",
    "\n",
    "print(f\"\\n✅ Found {len(top_k_complete_paths)} complete paths!\")\n",
    "print()\n",
    "\n",
    "# Display paths\n",
    "for rank, path in enumerate(top_k_complete_paths, 1):\n",
    "    node_descs = path.get_node_descriptions(graph, model.tokenizer)\n",
    "    node_types = path.get_node_types(graph)\n",
    "    \n",
    "    print(f\"Path #{rank} (Score: {path.final_score:.8f}, Length: {len(path)})\")\n",
    "    print(f\"  {' → '.join(node_types)}\")\n",
    "    print(f\"  Start: {node_descs[0]}\")\n",
    "    print(f\"  End: {node_descs[-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Export Paths to DataFrame\n",
    "\n",
    "Create a structured DataFrame with path details for analysis and export to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataframe(paths, graph, tokenizer):\n",
    "    \"\"\"Convert list of Path objects to pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for rank, path in enumerate(paths, 1):\n",
    "        node_types = path.get_node_types(graph)\n",
    "        node_descs = path.get_node_descriptions(graph, tokenizer)\n",
    "        \n",
    "        rows.append({\n",
    "            'rank': rank,\n",
    "            'influence_score': path.score,\n",
    "            'length': len(path),\n",
    "            'start_type': node_types[0],\n",
    "            'end_type': node_types[-1],\n",
    "            'start_description': node_descs[0],\n",
    "            'end_description': node_descs[-1],\n",
    "            'path_summary': ' → '.join(node_types),\n",
    "            'full_path': ' → '.join(node_descs),\n",
    "            'min_edge_weight': min(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'max_edge_weight': max(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'avg_edge_weight': sum(abs(w) for w in path.edges) / len(path.edges) if path.edges else 0.0,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_complete = paths_to_dataframe(complete_paths, graph, model.tokenizer)\n",
    "\n",
    "print(\"Complete Paths DataFrame:\")\n",
    "print(df_complete[['rank', 'influence_score', 'length', 'path_summary']])\n",
    "print()\n",
    "\n",
    "# Save to CSV\n",
    "df_complete.to_csv('complete_paths_austin.csv', index=False)\n",
    "print(\"✅ Saved to complete_paths_austin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Graph\n",
    "\n",
    "Save the attribution graph to a .pt file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as LibPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save graph\n",
    "graph_dir = LibPath('graphs')\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_name = 'dallas_capital_attribution.pt'\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "print(f\"Saving graph to {graph_path}...\")\n",
    "graph.to_pt(graph_path)\n",
    "print(f\"Graph saved successfully! (Size: {graph_path.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Visualization Files\n",
    "\n",
    "Generate graph files for interactive visualization. The pruning thresholds control how much of the graph to keep:\n",
    "- `node_threshold`: Keep minimum nodes whose cumulative influence >= this value\n",
    "- `edge_threshold`: Keep minimum edges whose cumulative influence >= this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-capital\"  # Name for this graph\n",
    "graph_file_dir = './graph_files'\n",
    "node_threshold = 0.8  # Keep nodes explaining 80% of influence\n",
    "edge_threshold = 0.98  # Keep edges explaining 98% of influence\n",
    "\n",
    "print(f\"Creating visualization files with slug '{slug}'...\")\n",
    "print(f\"Node threshold: {node_threshold}, Edge threshold: {edge_threshold}\")\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")\n",
    "\n",
    "print(f\"Visualization files created in {graph_file_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch Visualization Server\n",
    "\n",
    "Start a local server to interactively explore the attribution graph.\n",
    "\n",
    "**Features:**\n",
    "- Click to select nodes\n",
    "- Ctrl/Cmd+Click to pin/unpin nodes to your subgraph\n",
    "- G+Click on nodes to group them into supernodes\n",
    "- Edit node descriptions by clicking the edit button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visualization server on port 8046...\n",
      "\n",
      "Visualization server is running!\n",
      "Open your graph here: http://localhost:8046/index.html\n",
      "\n",
      "To stop the server later, run: server.stop()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://localhost:8046/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x78d66be369d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "from IPython.display import IFrame\n",
    "\n",
    "port = 8046\n",
    "print(f\"Starting visualization server on port {port}...\")\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "print(f\"\\nVisualization server is running!\")\n",
    "print(f\"Open your graph here: http://localhost:{port}/index.html\")\n",
    "print(f\"\\nTo stop the server later, run: server.stop()\")\n",
    "\n",
    "# Display in iframe\n",
    "display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stop the Server (Optional)\n",
    "\n",
    "Uncomment and run this cell when you're done with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()\n",
    "# print(\"Server stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-circuit-tracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
