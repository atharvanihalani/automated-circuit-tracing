{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Graph for Dallas Capital Query\n",
    "\n",
    "This notebook creates an attribution graph for the sentence:\n",
    "**\"Fact: The capital of the state containing Dallas is\"**\n",
    "\n",
    "We'll use the Gemma-2 (2B) model with GemmaScope transcoders to analyze the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "- Load Model and Transcoders\n",
    "- Configure Attribution Parameters\n",
    "- Run Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-2b with gemma transcoders...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f2c70f2eae4db48fc4d58e91033a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d2f505a42c4d40b3d370cfd3fd9c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"  # GemmaScope transcoders\n",
    "\n",
    "print(f\"Loading {model_name} with {transcoder_name} transcoders...\")\n",
    "model = ReplacementModel.from_pretrained(\n",
    "    model_name, \n",
    "    transcoder_name, \n",
    "    dtype=torch.bfloat16,\n",
    "    lazy_encoder=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: The capital of the state containing Dallas is\n",
      "Max logits: 10\n",
      "Desired logit probability: 0.95\n",
      "Max feature nodes: 8192\n",
      "Batch size: 256\n",
      "Offload strategy: cpu\n"
     ]
    }
   ],
   "source": [
    "# Attribution parameters\n",
    "prompt = \"Fact: The capital of the state containing Dallas is\"\n",
    "max_n_logits = 10\n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192  # None for no limit, but will be slower\n",
    "batch_size = 256\n",
    "offload = 'cpu'  # Use 'disk' if running out of memory, None to keep everything on GPU\n",
    "verbose = True\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Max logits: {max_n_logits}\")\n",
    "print(f\"Desired logit probability: {desired_logit_prob}\")\n",
    "print(f\"Max feature nodes: {max_feature_nodes}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Offload strategy: {offload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attribution...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputation completed in 0.31s\n",
      "Found 9081 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.09s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7695\n",
      "Will include 8192 of 9081 feature nodes\n",
      "Input vectors built in 1.59s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.39s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:08<00:00, 999.71it/s] \n",
      "Feature attributions completed in 8.20s\n",
      "Attribution completed in 16.68s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribution complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning attribution...\\n\")\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "print(\"\\nAttribution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display Graph Statistics\n",
    "\n",
    "Let's examine the structure of the attribution graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of active features: 9081\n",
      "length of adjacency matrix: 8499\n",
      "number of \"activation values\": 9081\n"
     ]
    }
   ],
   "source": [
    "print(f'number of active features: {len(graph.active_features)}')\n",
    "print(f'length of adjacency matrix: {len(graph.adjacency_matrix)}')\n",
    "print(f'number of \"activation values\": {len(graph.activation_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPH STATISTICS\n",
      "============================================================\n",
      "\n",
      "Input String: <bos>Fact: The capital of the state containing Dallas is\n",
      "Input Tokens: [2, 18143, 235292, 714, 6037, 576, 573, 2329, 10751, 26865, 603]\n",
      "Number of positions: 11\n",
      "\n",
      "Total active features: 9081\n",
      "Selected features for graph: 8192\n",
      "\n",
      "Graph Structure:\n",
      "  Feature nodes: 8192\n",
      "  Error nodes: 286 (26 layers × 11 positions)\n",
      "  Embedding nodes: 11\n",
      "  Logit nodes: 10\n",
      "  Total nodes: 8499\n",
      "\n",
      "Total non-zero edges: 19,022,999\n",
      "Adjacency matrix shape: torch.Size([8499, 8499])\n",
      "Adjacency matrix density: 26.34%\n",
      "\n",
      "Top 10 predicted logits:\n",
      "  1. ' Austin' (token 22605) - probability: 0.4453\n",
      "  2. ' not' (token 780) - probability: 0.0776\n",
      "  3. ' the' (token 573) - probability: 0.0532\n",
      "  4. ' Texas' (token 9447) - probability: 0.0415\n",
      "  5. ' Fort' (token 9778) - probability: 0.0366\n",
      "  6. ' Houston' (token 22898) - probability: 0.0286\n",
      "  7. ' Dallas' (token 26865) - probability: 0.0251\n",
      "  8. ' ' (token 235248) - probability: 0.0251\n",
      "  9. ' Oklahoma' (token 28239) - probability: 0.0197\n",
      "  10. ' San' (token 3250) - probability: 0.0153\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input information\n",
    "print(f\"\\nInput String: {graph.input_string}\")\n",
    "print(f\"Input Tokens: {graph.input_tokens.tolist()}\")\n",
    "print(f\"Number of positions: {graph.n_pos}\")\n",
    "\n",
    "# Feature information\n",
    "print(f\"\\nTotal active features: {len(graph.active_features)}\")\n",
    "print(f\"Selected features for graph: {len(graph.selected_features)}\")\n",
    "\n",
    "# Node structure\n",
    "n_layers = graph.cfg.n_layers\n",
    "n_pos = graph.n_pos\n",
    "n_error_nodes = n_layers * n_pos\n",
    "n_embed_nodes = n_pos\n",
    "n_logit_nodes = len(graph.logit_tokens)\n",
    "total_nodes = len(graph.selected_features) + n_error_nodes + n_embed_nodes + n_logit_nodes\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "print(f\"  Feature nodes: {len(graph.selected_features)}\")\n",
    "print(f\"  Error nodes: {n_error_nodes} ({n_layers} layers × {n_pos} positions)\")\n",
    "print(f\"  Embedding nodes: {n_embed_nodes}\")\n",
    "print(f\"  Logit nodes: {n_logit_nodes}\")\n",
    "print(f\"  Total nodes: {total_nodes}\")\n",
    "\n",
    "# Edge information\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "total_edges = (adjacency_matrix != 0).sum().item()\n",
    "print(f\"\\nTotal non-zero edges: {total_edges:,}\")\n",
    "print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n",
    "print(f\"Adjacency matrix density: {total_edges / (adjacency_matrix.shape[0] * adjacency_matrix.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "# Top logits\n",
    "print(f\"\\nTop {len(graph.logit_tokens)} predicted logits:\")\n",
    "for i, (token_id, prob) in enumerate(zip(graph.logit_tokens, graph.logit_probabilities)):\n",
    "    token_str = model.tokenizer.decode([token_id.item()])\n",
    "    print(f\"  {i+1}. '{token_str}' (token {token_id.item()}) - probability: {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topological Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_details(matrix_idx: int) -> tuple[int, int, int]:\n",
    "    assert matrix_idx < len(graph.selected_features), 'This node is not an active feature'\n",
    "    feature_idx = graph.selected_features[matrix_idx]\n",
    "    layer, token_pos, attribution_idx = graph.active_features[feature_idx]\n",
    "\n",
    "    return (layer.item(), token_pos.item(), attribution_idx.item())\n",
    "\n",
    "def matrix_idx_to_explanation(matrix_idx: int):\n",
    "    layer, __, feature_idx = get_feature_details(matrix_idx)\n",
    "\n",
    "    url = f'https://www.neuronpedia.org/gemma-2-2b/{layer}-gemmascope-transcoder-16k/{feature_idx}'\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "    body = soup.find('html').find('body')\n",
    "    idx_a = str(body).find('explanationModelName')\n",
    "    target_substring_large = str(body)[idx_a-200:idx_a]\n",
    "    assert 'description' in target_substring_large\n",
    "\n",
    "    idx_b = target_substring_large.find('description')\n",
    "    const_1 = 16\n",
    "    const_2 = 5\n",
    "    target_substring_final = target_substring_large[idx_b + const_1: -const_2]\n",
    "\n",
    "    return target_substring_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topological_order(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute topological order using Kahn's algorithm.\n",
    "    \n",
    "    Args:\n",
    "        adjacency_matrix: torch.Tensor of shape (n_nodes, n_nodes)\n",
    "                         where adjacency_matrix[target, source] represents edge from source -> target\n",
    "    \n",
    "    Returns:\n",
    "        list: Topological order of node indices\n",
    "    \"\"\"\n",
    "    n_nodes = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Compute in-degrees: for each node, count how many edges point TO it\n",
    "    in_degree = (adjacency_matrix != 0).sum(dim=1).cpu()\n",
    "    \n",
    "    # Initialize queue with nodes that have no incoming edges\n",
    "    queue = [i for i in range(n_nodes) if in_degree[i] == 0]\n",
    "    topo_order = []\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        topo_order.append(node)\n",
    "        \n",
    "        # For each outgoing edge from this node\n",
    "        outgoing_edges = (adjacency_matrix[:, node] != 0).cpu()\n",
    "        \n",
    "        for target in range(n_nodes):\n",
    "            if outgoing_edges[target]:\n",
    "                in_degree[target] -= 1\n",
    "                if in_degree[target] == 0:\n",
    "                    queue.append(target)\n",
    "    \n",
    "    if len(topo_order) != n_nodes:\n",
    "        print(f\"Warning: Graph contains a cycle! Only {len(topo_order)}/{n_nodes} nodes ordered.\")\n",
    "        remaining = [i for i in range(n_nodes) if i not in topo_order]\n",
    "        topo_order.extend(remaining)\n",
    "    \n",
    "    return topo_order\n",
    "\n",
    "def get_adjacency_without_error_nodes(adjacency_matrix):\n",
    "    # remove the error nodes from the adjacency matrix!\n",
    "    n_features = len(graph.selected_features)\n",
    "    n_error_nodes = len(graph.input_tokens) * model.cfg.n_layers\n",
    "\n",
    "    mask = torch.ones_like(adjacency_matrix[0]).to(torch.bool)\n",
    "    mask[n_features : n_features + n_error_nodes] = False\n",
    "    updated_matrix = adjacency_matrix[mask][:, mask]\n",
    "\n",
    "    return updated_matrix\n",
    "\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "\n",
    "print(\"Computing topological order of the attribution graph...\")\n",
    "topo_order = compute_topological_order(adjacency_matrix)\n",
    "print(f\"Topological order computed: {len(topo_order)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_details(node: int):\n",
    "    '''\n",
    "    layers range from -1 to 26  \n",
    "    layer -1 is the embedding  \n",
    "    layer 26 is the logits  \n",
    "\n",
    "    token positions range from 1 to 10  \n",
    "    (BOS is token 0; it is excluded)\n",
    "    '''\n",
    "    n_features = len(graph.selected_features)\n",
    "    n_error_nodes = len(graph.input_tokens) * model.cfg.n_layers\n",
    "    n_embed_nodes = len(graph.input_tokens)\n",
    "    n_logit_nodes = len(graph.logit_tokens)\n",
    "\n",
    "    if node < n_features:\n",
    "        layer, token_pos, __ = get_feature_details(node)\n",
    "    elif node < (n_features + n_error_nodes):\n",
    "        error_number = node - n_features\n",
    "        token_pos = error_number % 11\n",
    "        layer = error_number // 11\n",
    "    elif node < (n_features + n_error_nodes + n_embed_nodes):\n",
    "        layer = -1\n",
    "        token_pos = node - (n_features + n_error_nodes)\n",
    "    else:\n",
    "        layer = model.cfg.n_layers\n",
    "        token_pos = len(graph.input_tokens) - 1\n",
    "\n",
    "    return layer, token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topological sort is okay!\n"
     ]
    }
   ],
   "source": [
    "def test_topological_sort(topological_sort, n_error_nodes=286):\n",
    "    '''\n",
    "    the only 'illegal moves' are  \n",
    "    (a) moving to same token, previous layer  \n",
    "    (b) moving to previous token, previous layer  \n",
    "    '''\n",
    "\n",
    "    prev_layer = -1\n",
    "    prev_token_pos = 0\n",
    "\n",
    "    for idx, node in enumerate(topological_sort[286:]):\n",
    "        layer, token_pos = get_node_details(node)\n",
    "        if (layer < prev_layer) and (token_pos <= prev_token_pos):\n",
    "            print(f'error in topological sort at idx: {idx}')\n",
    "        \n",
    "    print(f'topological sort is okay!')\n",
    "\n",
    "test_topological_sort(topo_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find top-k most influential paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path finding functions loaded (optimized version)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from circuit_tracer.graph import compute_node_influence\n",
    "\n",
    "@dataclass\n",
    "class Path:\n",
    "    \"\"\"Represents a path through the attribution graph.\"\"\"\n",
    "    nodes: List[int]\n",
    "    edges: List[float]\n",
    "    score: float\n",
    "    # averaged_score: float\n",
    "    final_score: float\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    \n",
    "    def get_node_types(self, graph) -> List[str]:\n",
    "        \"\"\"Return node types: 'feature', 'error', 'embed', 'logit'.\"\"\"\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        types = []\n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                types.append('feature')\n",
    "            elif node < n_features + n_errors:\n",
    "                types.append('error')\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                types.append('embed')\n",
    "            else:\n",
    "                types.append('logit')\n",
    "        return types\n",
    "    \n",
    "    def get_node_descriptions(self, graph, tokenizer) -> List[str]:\n",
    "        \"\"\"Return human-readable descriptions for each node.\"\"\"\n",
    "        descriptions = []\n",
    "        n_features = len(graph.selected_features)\n",
    "        n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "        n_embeds = graph.n_pos\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            if node < n_features:\n",
    "                layer, pos, feat_idx = graph.active_features[graph.selected_features[node]].tolist()\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Feature L{layer}:F{feat_idx} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors:\n",
    "                error_idx = node - n_features\n",
    "                layer = error_idx // graph.n_pos\n",
    "                pos = error_idx % graph.n_pos\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Error L{layer} @ pos {pos} ('{token}')\")\n",
    "            elif node < n_features + n_errors + n_embeds:\n",
    "                pos = node - n_features - n_errors\n",
    "                token = tokenizer.decode([graph.input_tokens[pos]])\n",
    "                descriptions.append(f\"Embedding @ pos {pos} ('{token}')\")\n",
    "            else:\n",
    "                logit_idx = node - n_features - n_errors - n_embeds\n",
    "                token = tokenizer.decode([graph.logit_tokens[logit_idx]])\n",
    "                prob = graph.logit_probabilities[logit_idx].item()\n",
    "                descriptions.append(f\"Logit '{token}' (p={prob:.4f})\")\n",
    "        \n",
    "        return descriptions\n",
    "\n",
    "\n",
    "def find_k_best_paths_dp(adj_matrix, source_nodes, sink_node, topo_order, k=10, verbose=True):\n",
    "    \"\"\"OPTIMIZED: Find top-K paths using DP. Stores lightweight references, reconstructs at end.\"\"\"\n",
    "    best_path_refs = {}\n",
    "    best_path_refs[sink_node] = [(None, None, 1.0)]\n",
    "    \n",
    "    iterator = reversed(topo_order) if not verbose else tqdm(\n",
    "        reversed(topo_order), desc=\"DP path finding (optimized)\", total=len(topo_order)\n",
    "    )\n",
    "    \n",
    "    for node in iterator:\n",
    "        if node == sink_node:\n",
    "            continue\n",
    "        \n",
    "        outgoing_weights = adj_matrix[:, node]\n",
    "        successors = torch.where(outgoing_weights != 0)[0]\n",
    "        \n",
    "        if len(successors) == 0:\n",
    "            best_path_refs[node] = []\n",
    "            continue\n",
    "        \n",
    "        candidate_refs = []\n",
    "        for succ in successors:\n",
    "            succ_idx = succ.item()\n",
    "            if succ_idx not in best_path_refs or len(best_path_refs[succ_idx]) == 0:\n",
    "                continue\n",
    "            \n",
    "            edge_weight = outgoing_weights[succ].item()\n",
    "            for succ_next, succ_edge, path_score in best_path_refs[succ_idx]:\n",
    "                new_score = abs(edge_weight) * path_score\n",
    "                candidate_refs.append((succ_idx, edge_weight, new_score))\n",
    "        \n",
    "        candidate_refs.sort(key=lambda x: x[2], reverse=True)\n",
    "        best_path_refs[node] = candidate_refs[:k]\n",
    "    \n",
    "    def reconstruct_path(start_node, path_ref_index):\n",
    "        \"\"\"Reconstruct full path by following successor chain.\"\"\"\n",
    "        nodes, edges = [start_node], []\n",
    "        current_node, current_ref_idx = start_node, path_ref_index\n",
    "        \n",
    "        while True:\n",
    "            next_node, edge_weight, score = best_path_refs[current_node][current_ref_idx]\n",
    "            if next_node is None:\n",
    "                break\n",
    "            \n",
    "            nodes.append(next_node)\n",
    "            edges.append(edge_weight)\n",
    "            current_node = next_node\n",
    "            \n",
    "            target_score = score / abs(edge_weight)\n",
    "            current_ref_idx = 0\n",
    "            for i, (nn, ne, ns) in enumerate(best_path_refs[current_node]):\n",
    "                if abs(ns - target_score) < 1e-9:\n",
    "                    current_ref_idx = i\n",
    "                    break\n",
    "        \n",
    "        return Path(nodes=nodes, edges=edges, score=best_path_refs[start_node][path_ref_index][2], final_score=0.0)\n",
    "    \n",
    "    all_source_paths = []\n",
    "    for source in source_nodes:\n",
    "        if source in best_path_refs and len(best_path_refs[source]) > 0:\n",
    "            for path_idx in range(len(best_path_refs[source])):\n",
    "                all_source_paths.append(reconstruct_path(source, path_idx))\n",
    "\n",
    "    return all_source_paths\n",
    "    \n",
    "    # for path in all_source_paths:\n",
    "    #     path.averaged_score = path.score / len(path.edges)\n",
    "    #     path.final_score = sum(path.edges) / len(path.edges)\n",
    "    \n",
    "    # all_source_paths.sort(key=lambda p: p.final_score, reverse=True)\n",
    "    # return all_source_paths[:k]\n",
    "\n",
    "\n",
    "print(\"✅ Path finding functions loaded (optimized version)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top-10 complete paths: Embeddings [8478:8489] → Austin [8489]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DP path finding (optimized): 100%|██████████| 8499/8499 [02:44<00:00, 51.54it/s] \n"
     ]
    }
   ],
   "source": [
    "# Define node indices\n",
    "n_features = len(graph.selected_features)\n",
    "n_errors = graph.cfg.n_layers * graph.n_pos\n",
    "n_embeds = graph.n_pos\n",
    "n_logits = len(graph.logit_tokens)\n",
    "\n",
    "embed_start = n_features + n_errors\n",
    "embed_end = embed_start + n_embeds\n",
    "embed_nodes = list(range(embed_start, embed_end))\n",
    "austin_logit = embed_end  # Index 8489\n",
    "\n",
    "print(f\"Finding top-10 complete paths: Embeddings [{embed_start}:{embed_end}] → Austin [{austin_logit}]\")\n",
    "print()\n",
    "\n",
    "# Find complete paths\n",
    "all_complete_paths = find_k_best_paths_dp(\n",
    "    adj_matrix=adjacency_matrix,\n",
    "    source_nodes=embed_nodes,\n",
    "    sink_node=austin_logit,\n",
    "    topo_order=topo_order,\n",
    "    k=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "for path in all_complete_paths:\n",
    "    mylen = len(path.edges)\n",
    "    if mylen <= 24:\n",
    "        print(mylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = all_complete_paths[0]\n",
    "# print(f'len graph active featrues: {len(graph.active_features)}')\n",
    "\n",
    "for node in mypath.nodes:\n",
    "    # print(f'node: {node}')\n",
    "    try:\n",
    "        node_explanation = matrix_idx_to_explanation(node)\n",
    "        print(node_explanation)\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in all_complete_paths:\n",
    "    path.final_score = sum(path.edges) / len(path.edges)\n",
    "\n",
    "all_complete_paths_sorted = sorted(all_complete_paths, key=lambda p: p.final_score, reverse=True)\n",
    "top_k_complete_paths = all_complete_paths_sorted[:10]\n",
    "\n",
    "print(f\"\\n✅ Found {len(top_k_complete_paths)} complete paths!\")\n",
    "print()\n",
    "\n",
    "# Display paths\n",
    "for rank, path in enumerate(top_k_complete_paths, 1):\n",
    "    node_descs = path.get_node_descriptions(graph, model.tokenizer)\n",
    "    node_types = path.get_node_types(graph)\n",
    "    \n",
    "    print(f\"Path #{rank} (Score: {path.final_score:.8f}, Length: {len(path)})\")\n",
    "    print(f\"  {' → '.join(node_types)}\")\n",
    "    print(f\"  Start: {node_descs[0]}\")\n",
    "    print(f\"  End: {node_descs[-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Paths to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataframe(paths, graph, tokenizer):\n",
    "    \"\"\"Convert list of Path objects to pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for rank, path in enumerate(paths, 1):\n",
    "        node_types = path.get_node_types(graph)\n",
    "        node_descs = path.get_node_descriptions(graph, tokenizer)\n",
    "        \n",
    "        rows.append({\n",
    "            'rank': rank,\n",
    "            'influence_score': path.score,\n",
    "            'length': len(path),\n",
    "            'start_type': node_types[0],\n",
    "            'end_type': node_types[-1],\n",
    "            'start_description': node_descs[0],\n",
    "            'end_description': node_descs[-1],\n",
    "            'path_summary': ' → '.join(node_types),\n",
    "            'full_path': ' → '.join(node_descs),\n",
    "            'min_edge_weight': min(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'max_edge_weight': max(abs(w) for w in path.edges) if path.edges else 0.0,\n",
    "            'avg_edge_weight': sum(abs(w) for w in path.edges) / len(path.edges) if path.edges else 0.0,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_complete = paths_to_dataframe(complete_paths, graph, model.tokenizer)\n",
    "\n",
    "print(\"Complete Paths DataFrame:\")\n",
    "print(df_complete[['rank', 'influence_score', 'length', 'path_summary']])\n",
    "print()\n",
    "\n",
    "# Save to CSV\n",
    "df_complete.to_csv('complete_paths_austin.csv', index=False)\n",
    "print(\"✅ Saved to complete_paths_austin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as LibPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to graphs/dallas_capital_attribution.pt...\n",
      "Graph saved successfully! (Size: 275.84 MB)\n"
     ]
    }
   ],
   "source": [
    "# Create output directory and save graph\n",
    "graph_dir = LibPath('graphs')\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_name = 'dallas_capital_attribution.pt'\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "print(f\"Saving graph to {graph_path}...\")\n",
    "graph.to_pt(graph_path)\n",
    "print(f\"Graph saved successfully! (Size: {graph_path.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate graph files for interactive visualization. The pruning thresholds control how much of the graph to keep:\n",
    "- `node_threshold`: Keep minimum nodes whose cumulative influence >= this value\n",
    "- `edge_threshold`: Keep minimum edges whose cumulative influence >= this value\n",
    "\n",
    "**Graph Features:**\n",
    "- Click to select nodes\n",
    "- Ctrl/Cmd+Click to pin/unpin nodes to your subgraph\n",
    "- G+Click on nodes to group them into supernodes\n",
    "- Edit node descriptions by clicking the edit button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization files with slug 'dallas-capital'...\n",
      "Node threshold: 0.8, Edge threshold: 0.98\n",
      "Visualization files created in ./graph_files/\n"
     ]
    }
   ],
   "source": [
    "slug = \"dallas-capital\"  # Name for this graph\n",
    "graph_file_dir = './graph_files'\n",
    "node_threshold = 0.8  # Keep nodes explaining 80% of influence\n",
    "edge_threshold = 0.98  # Keep edges explaining 98% of influence\n",
    "\n",
    "print(f\"Creating visualization files with slug '{slug}'...\")\n",
    "print(f\"Node threshold: {node_threshold}, Edge threshold: {edge_threshold}\")\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")\n",
    "\n",
    "print(f\"Visualization files created in {graph_file_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visualization server on port 8047...\n",
      "\n",
      "Visualization server is running!\n",
      "Open your graph here: http://localhost:8047/index.html\n",
      "\n",
      "To stop the server later, run: server.stop()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://localhost:8047/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x762b071b3790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "from IPython.display import IFrame\n",
    "\n",
    "port = 8047\n",
    "print(f\"Starting visualization server on port {port}...\")\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "print(f\"\\nVisualization server is running!\")\n",
    "print(f\"Open your graph here: http://localhost:{port}/index.html\")\n",
    "print(f\"\\nTo stop the server later, run: server.stop()\")\n",
    "\n",
    "# Display in iframe\n",
    "display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()\n",
    "# print(\"Server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-circuit-tracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
