{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Graph for Dallas Capital Query\n",
    "\n",
    "This notebook creates an attribution graph for the sentence:\n",
    "**\"Fact: The capital of the state containing Dallas is\"**\n",
    "\n",
    "We'll use the Gemma-2 (2B) model with GemmaScope transcoders to analyze the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch as t\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "import einops\n",
    "import torch as t\n",
    "from math import perm, comb\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model + run attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-2b with gemma transcoders...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d939ea634847adb04e5ce91d71e556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fbc9c7e78c46a790a34d3586d54393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"  # GemmaScope transcoders\n",
    "\n",
    "print(f\"Loading {model_name} with {transcoder_name} transcoders...\")\n",
    "model = ReplacementModel.from_pretrained(\n",
    "    model_name, \n",
    "    transcoder_name, \n",
    "    dtype=t.bfloat16,\n",
    "    lazy_encoder=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: The capital of the state containing Dallas is\n",
      "Max logits: 10\n",
      "Desired logit probability: 0.95\n",
      "Max feature nodes: 8192\n",
      "Batch size: 256\n",
      "Offload strategy: cpu\n"
     ]
    }
   ],
   "source": [
    "# Attribution parameters\n",
    "prompt = \"Fact: The capital of the state containing Dallas is\"\n",
    "max_n_logits = 10\n",
    "desired_logit_prob = 0.95\n",
    "max_feature_nodes = 8192  # None for no limit, but will be slower\n",
    "batch_size = 256\n",
    "offload = 'cpu'  # Use 'disk' if running out of memory, None to keep everything on GPU\n",
    "verbose = True\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Max logits: {max_n_logits}\")\n",
    "print(f\"Desired logit probability: {desired_logit_prob}\")\n",
    "print(f\"Max feature nodes: {max_feature_nodes}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Offload strategy: {offload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attribution...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputation completed in 0.41s\n",
      "Found 9081 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.08s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7695\n",
      "Will include 8192 of 9081 feature nodes\n",
      "Input vectors built in 1.25s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.08s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:02<00:00, 3312.38it/s]\n",
      "Feature attributions completed in 2.48s\n",
      "Attribution completed in 8.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribution complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning attribution...\\n\")\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")\n",
    "print(\"\\nAttribution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as LibPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory and save graph\n",
    "graph_dir = LibPath('graphs')\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "graph_name = 'dallas_capital_attribution.pt'\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "print(f\"Saving graph to {graph_path}...\")\n",
    "graph.to_pt(graph_path)\n",
    "print(f\"Graph saved successfully! (Size: {graph_path.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `node_threshold`: Keep minimum nodes whose cumulative influence >= this value\n",
    "- `edge_threshold`: Keep minimum edges whose cumulative influence >= this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-capital\"  # Name for this graph\n",
    "graph_file_dir = './graph_files'\n",
    "node_threshold = 0.8  # Keep nodes explaining 80% of influence\n",
    "edge_threshold = 0.98  # Keep edges explaining 98% of influence\n",
    "\n",
    "print(f\"Creating visualization files with slug '{slug}'...\")\n",
    "print(f\"Node threshold: {node_threshold}, Edge threshold: {edge_threshold}\")\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")\n",
    "\n",
    "print(f\"Visualization files created in {graph_file_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "from IPython.display import IFrame\n",
    "\n",
    "port = 8047\n",
    "print(f\"Starting visualization server on port {port}...\")\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "print(f\"\\nVisualization server is running!\")\n",
    "print(f\"Open your graph here: http://localhost:{port}/index.html\")\n",
    "print(f\"\\nTo stop the server later, run: server.stop()\")\n",
    "\n",
    "# Display in iframe\n",
    "display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()\n",
    "# print(\"Server stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Graph Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of active features: {len(graph.active_features)}')\n",
    "print(f'length of adjacency matrix: {len(graph.adjacency_matrix)}')\n",
    "print(f'number of \"activation values\": {len(graph.activation_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Input information\n",
    "print(f\"\\nInput String: {graph.input_string}\")\n",
    "print(f\"Input Tokens: {graph.input_tokens.tolist()}\")\n",
    "print(f\"Number of positions: {graph.n_pos}\")\n",
    "\n",
    "# Feature information\n",
    "print(f\"\\nTotal active features: {len(graph.active_features)}\")\n",
    "print(f\"Selected features for graph: {len(graph.selected_features)}\")\n",
    "\n",
    "# Node structure\n",
    "n_layers = graph.cfg.n_layers\n",
    "n_pos = graph.n_pos\n",
    "n_error_nodes = n_layers * n_pos\n",
    "n_embed_nodes = n_pos\n",
    "n_logit_nodes = len(graph.logit_tokens)\n",
    "total_nodes = len(graph.selected_features) + n_error_nodes + n_embed_nodes + n_logit_nodes\n",
    "\n",
    "print(f\"\\nGraph Structure:\")\n",
    "print(f\"  Feature nodes: {len(graph.selected_features)}\")\n",
    "print(f\"  Error nodes: {n_error_nodes} ({n_layers} layers × {n_pos} positions)\")\n",
    "print(f\"  Embedding nodes: {n_embed_nodes}\")\n",
    "print(f\"  Logit nodes: {n_logit_nodes}\")\n",
    "print(f\"  Total nodes: {total_nodes}\")\n",
    "\n",
    "# Edge information\n",
    "adjacency_matrix = graph.adjacency_matrix\n",
    "total_edges = (adjacency_matrix != 0).sum().item()\n",
    "print(f\"\\nTotal non-zero edges: {total_edges:,}\")\n",
    "print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n",
    "print(f\"Adjacency matrix density: {total_edges / (adjacency_matrix.shape[0] * adjacency_matrix.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "# Top logits\n",
    "print(f\"\\nTop {len(graph.logit_tokens)} predicted logits:\")\n",
    "for i, (token_id, prob) in enumerate(zip(graph.logit_tokens, graph.logit_probabilities)):\n",
    "    token_str = model.tokenizer.decode([token_id.item()])\n",
    "    print(f\"  {i+1}. '{token_str}' (token {token_id.item()}) - probability: {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Number of Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_paths_length_base(i, n_tokens):\n",
    "    if i == 2:\n",
    "        return n_tokens\n",
    "\n",
    "    final_length = 0\n",
    "    for b_prime in range(n_tokens, 0, -1):\n",
    "        final_length += _calculate_paths_length_base(i-1, b_prime)\n",
    "    \n",
    "    return final_length\n",
    "\n",
    "\n",
    "def calculate_paths_simple_graph(layers, n_tokens, max_path_length=None) -> list[int]:\n",
    "    '''\n",
    "    Calculate the number of possible paths from source to sink for a 'simple attribution graph DAG'.\n",
    "    \n",
    "    :param layers: The number of layers in the model, including the embeddings & logits.\n",
    "    :param n_tokens: The number of input tokens (ie. the 'base' of the attribution graph)\n",
    "\n",
    "    :returns: A list with the number of complete paths of increasing lengths (from 2 to max_path_len, inclusive)\n",
    "    '''\n",
    "    \n",
    "    if max_path_length is None:\n",
    "        max_path_length = layers\n",
    "    assert max_path_length <= layers\n",
    "    \n",
    "    def calculate_paths_length_i(i):\n",
    "        factor = comb(layers-2, i-2)\n",
    "        length = _calculate_paths_length_base(i, n_tokens)\n",
    "        return factor * length\n",
    "\n",
    "    all_paths = []\n",
    "    for i in range(2, max_path_length + 1):\n",
    "        paths_length_i = calculate_paths_length_i(i)\n",
    "        all_paths.append(paths_length_i)\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "all_paths = calculate_paths_simple_graph(26, 10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate + Visualize Dummy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix_simple(n, b):\n",
    "    total_nodes = n*b + 1\n",
    "    base_matrix = einops.rearrange(t.arange(total_nodes-1), '(n b) -> n b', b=b)\n",
    "    adjacency_matrix = t.zeros([total_nodes, total_nodes])\n",
    "\n",
    "    for layer, nodes in enumerate(base_matrix):\n",
    "        for token_pos, node in enumerate(nodes):\n",
    "            # print(f'layer: {layer}')\n",
    "            # print(f'token pos: {token_pos}')\n",
    "            # print(f'node: {node}')\n",
    "\n",
    "            layers_left = t.arange(layer+1, n)\n",
    "            tokens_left = t.arange(token_pos, b)\n",
    "            nodes_left_coords = t.cartesian_prod(layers_left, tokens_left)\n",
    "            \n",
    "            rows = nodes_left_coords[:, 0]\n",
    "            cols = nodes_left_coords[:, 1]\n",
    "            nodes_left = base_matrix[rows, cols]\n",
    "            \n",
    "            # print(f'coordinates of nodes left: {nodes_left_coords}')\n",
    "            # print(f'nodes left: {nodes_left}')\n",
    "            # print()\n",
    "\n",
    "            adjacency_matrix[:, node][nodes_left] = 1\n",
    "\n",
    "    adjacency_matrix[-1, :-1] = 1\n",
    "\n",
    "\n",
    "    test_n = list(range(n))\n",
    "    test_b = list(range(b))\n",
    "    test_c = [(n_i, b_i) for n_i in test_n for b_i in test_b]\n",
    "\n",
    "    node_info = dict(enumerate(test_c))\n",
    "    node_info[total_nodes-1] = (n, b-1)\n",
    "\n",
    "\n",
    "    return adjacency_matrix, node_info\n",
    "\n",
    "adjacency_matrix, node_info = generate_adjacency_matrix_simple(n=25, b=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interactive visualization of attribution graphs with hover functionality.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_node_label(node_idx: int, node_info: dict) -> str:\n",
    "    \"\"\"Generate a human-readable label for a node.\"\"\"\n",
    "    layer, pos = node_info[node_idx]\n",
    "    total_layers = max(l for l, _ in node_info.values())\n",
    "\n",
    "    if layer == total_layers:  # Sink node\n",
    "        return f\"Output\"\n",
    "    elif layer == 0:\n",
    "        return f\"Input[{pos}]\"\n",
    "    else:\n",
    "        return f\"L{layer}[{pos}]\"\n",
    "\n",
    "def visualize_attribution_graph(\n",
    "    adj_matrix: np.ndarray,\n",
    "    node_info: dict,\n",
    "    title: str = \"Attribution Graph\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization of the attribution graph.\n",
    "\n",
    "    Hovering on nodes highlights:\n",
    "    - Nodes it feeds into (outgoing edges) in green\n",
    "    - Nodes it's influenced by (incoming edges) in blue\n",
    "    \"\"\"\n",
    "    n_nodes = adj_matrix.shape[0]\n",
    "\n",
    "    # Compute node positions (layer determines y, position determines x)\n",
    "    node_positions = {}\n",
    "    for node_idx, (layer, pos) in node_info.items():\n",
    "        # x: position (with spacing), y: layer (with spacing)\n",
    "        node_positions[node_idx] = (pos * 100, layer * 100)\n",
    "\n",
    "    # Create edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_hover_text = []\n",
    "\n",
    "    for target_idx in range(n_nodes):\n",
    "        for source_idx in range(n_nodes):\n",
    "            if adj_matrix[target_idx, source_idx] > 0:\n",
    "                x0, y0 = node_positions[source_idx]\n",
    "                x1, y1 = node_positions[target_idx]\n",
    "\n",
    "                edge_x.extend([x0, x1, None])\n",
    "                edge_y.extend([y0, y1, None])\n",
    "\n",
    "                source_label = get_node_label(source_idx, node_info)\n",
    "                target_label = get_node_label(target_idx, node_info)\n",
    "                edge_hover_text.append(f\"{source_label} → {target_label}\")\n",
    "\n",
    "    # Edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode='lines',\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Prepare node data\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_hover_info = []\n",
    "\n",
    "    for node_idx in range(n_nodes):\n",
    "        x, y = node_positions[node_idx]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "\n",
    "        label = get_node_label(node_idx, node_info)\n",
    "        node_text.append(label)\n",
    "\n",
    "        # Find incoming and outgoing edges\n",
    "        incoming_nodes = [i for i in range(n_nodes) if adj_matrix[node_idx, i] > 0]\n",
    "        outgoing_nodes = [i for i in range(n_nodes) if adj_matrix[i, node_idx] > 0]\n",
    "\n",
    "        incoming_labels = [get_node_label(i, node_info) for i in incoming_nodes]\n",
    "        outgoing_labels = [get_node_label(i, node_info) for i in outgoing_nodes]\n",
    "\n",
    "        hover_text = f\"<b>{label}</b><br>\"\n",
    "        hover_text += f\"<br><b>Influenced by ({len(incoming_nodes)}):</b><br>\"\n",
    "        hover_text += \"<br>\".join(incoming_labels[:10])  # Limit to first 10\n",
    "        if len(incoming_labels) > 10:\n",
    "            hover_text += f\"<br>... and {len(incoming_labels) - 10} more\"\n",
    "\n",
    "        hover_text += f\"<br><br><b>Feeds into ({len(outgoing_nodes)}):</b><br>\"\n",
    "        hover_text += \"<br>\".join(outgoing_labels[:10])\n",
    "        if len(outgoing_labels) > 10:\n",
    "            hover_text += f\"<br>... and {len(outgoing_labels) - 10} more\"\n",
    "\n",
    "        node_hover_info.append(hover_text)\n",
    "\n",
    "    # Node trace\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=20,\n",
    "            color='lightblue',\n",
    "            line=dict(width=2, color='darkblue')\n",
    "        ),\n",
    "        text=node_text,\n",
    "        textposition=\"top center\",\n",
    "        textfont=dict(size=10),\n",
    "        hovertext=node_hover_info,\n",
    "        hoverinfo='text',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        xaxis=dict(\n",
    "            title='Token Position',\n",
    "            showgrid=True,\n",
    "            zeroline=False,\n",
    "            showticklabels=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Layer',\n",
    "            showgrid=True,\n",
    "            zeroline=False,\n",
    "            showticklabels=True\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "print(\"\\nCreating interactive visualization...\")\n",
    "fig = visualize_attribution_graph(\n",
    "    adjacency_matrix,\n",
    "    node_info,\n",
    ")\n",
    "\n",
    "fig.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(graph.selected_features)\n",
    "n_error_nodes = len(graph.input_tokens) * model.cfg.n_layers\n",
    "n_embed_nodes = len(graph.input_tokens)\n",
    "n_logit_nodes = len(graph.logit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4453, 0.0776, 0.0532, 0.0415, 0.0366, 0.0286, 0.0251, 0.0251, 0.0197,\n",
       "        0.0153], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.logit_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_details(matrix_idx: int) -> tuple[int, int, int]:\n",
    "    assert matrix_idx < len(graph.selected_features), 'This node is not an active feature'\n",
    "    feature_idx = graph.selected_features[matrix_idx]\n",
    "    layer, token_pos, attribution_idx = graph.active_features[feature_idx]\n",
    "\n",
    "    return (layer.item(), token_pos.item(), attribution_idx.item())\n",
    "\n",
    "def get_node_details(node: int) -> tuple[int, int]:\n",
    "    '''\n",
    "    get the layer and token pos of any node in the adjacency matrix.\n",
    "    could be a feature, an error, an embedding, or a logit node.\n",
    "\n",
    "    layers range from -1 to 26  \n",
    "    layer -1 is the embedding  \n",
    "    layer 26 is the logits  \n",
    "\n",
    "    token positions range from 1 to 10  \n",
    "    (BOS is token 0; it is excluded)\n",
    "    '''\n",
    "    n_features = len(graph.selected_features)\n",
    "    n_error_nodes = len(graph.input_tokens) * model.cfg.n_layers\n",
    "    n_embed_nodes = len(graph.input_tokens)\n",
    "    n_logit_nodes = len(graph.logit_tokens)\n",
    "\n",
    "    if node < n_features:\n",
    "        layer, token_pos, __ = get_feature_details(node)\n",
    "    elif node < (n_features + n_error_nodes):\n",
    "        error_number = node - n_features\n",
    "        token_pos = error_number % 11\n",
    "        layer = error_number // 11\n",
    "    elif node < (n_features + n_error_nodes + n_embed_nodes):\n",
    "        layer = -1\n",
    "        token_pos = node - (n_features + n_error_nodes)\n",
    "    else:\n",
    "        layer = model.cfg.n_layers\n",
    "        token_pos = len(graph.input_tokens) - 1\n",
    "\n",
    "    return layer, token_pos\n",
    "\n",
    "def matrix_idx_to_explanation(matrix_idx: int):\n",
    "    is_feature = matrix_idx < n_features\n",
    "    is_error = n_features <= matrix_idx < (n_features + n_error_nodes)\n",
    "    is_embed = (n_features + n_error_nodes) <= matrix_idx < (n_features + n_error_nodes + n_embed_nodes)\n",
    "    is_logit = (n_features + n_error_nodes + n_embed_nodes) <= matrix_idx\n",
    "    \n",
    "    if is_feature:\n",
    "        layer, __, feature_idx = get_feature_details(matrix_idx)\n",
    "\n",
    "        url = f'https://www.neuronpedia.org/gemma-2-2b/{layer}-gemmascope-transcoder-16k/{feature_idx}'\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "        body = soup.find('html').find('body')\n",
    "        idx_a = str(body).find('explanationModelName')\n",
    "        target_substring_large = str(body)[idx_a-200:idx_a]\n",
    "        assert 'description' in target_substring_large\n",
    "\n",
    "        idx_b = target_substring_large.find('description')\n",
    "        const_1 = 16\n",
    "        const_2 = 5\n",
    "        target_substring_final = target_substring_large[idx_b + const_1: -const_2]\n",
    "\n",
    "        return target_substring_final\n",
    "    elif is_error:\n",
    "        return 'error node – this should NOT be possible'\n",
    "    elif is_embed:\n",
    "        embed_num = matrix_idx - (n_features + n_error_nodes)\n",
    "        embed_id = graph.input_tokens[embed_num]\n",
    "        embed_token = model.tokenizer.decode(embed_id)\n",
    "\n",
    "        return f'embed: {embed_token}'\n",
    "    elif is_logit:\n",
    "        logit_num = matrix_idx - (n_features + n_error_nodes + n_embed_nodes)\n",
    "        logit_id = graph.logit_tokens[logit_num]\n",
    "        logit_token = model.tokenizer.decode(logit_id)\n",
    "\n",
    "        return f'logit: {logit_token} ({graph.logit_probabilities[logit_num]})'\n",
    "    else:\n",
    "        raise IndexError('Incorrect Matrix Idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_paths_wrapper(adj_matrix, start_tokens, end_token, max_path_length=None):\n",
    "    all_paths = []\n",
    "\n",
    "    def find_all_paths(current_path):\n",
    "        assert len(current_path) >= 1\n",
    "        current_node = current_path[-1]\n",
    "\n",
    "        if current_node == end_token:\n",
    "            all_paths.append(current_path)\n",
    "            return\n",
    "        if (max_path_length is not None) and (len(current_path) >= max_path_length):\n",
    "            return\n",
    "        \n",
    "        mystack = t.nonzero(adj_matrix[:, current_node])\n",
    "        for node in mystack:\n",
    "            find_all_paths(current_path + [node.item()])\n",
    "\n",
    "\n",
    "    for node in start_tokens:\n",
    "        current_path = [node]\n",
    "        find_all_paths(current_path)\n",
    "        pass\n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "def score_paths(paths, adjacency_matrix):\n",
    "    def score_path(path: list[int]):\n",
    "        score = 0\n",
    "        for source_node, target_node in zip(path[:-1], path[1:]):\n",
    "            score += adjacency_matrix[target_node, source_node].item()\n",
    "        score = score / (len(path) - 1)\n",
    "        return score\n",
    "    \n",
    "    all_scores = [score_path(p) for p in tqdm(paths)]\n",
    "\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55556/55556 [00:00<00:00, 173228.82it/s]\n"
     ]
    }
   ],
   "source": [
    "a = n_features + n_error_nodes\n",
    "b = n_features + n_error_nodes + n_embed_nodes\n",
    "\n",
    "all_paths = find_all_paths_wrapper(\n",
    "    graph.adjacency_matrix,\n",
    "    start_tokens = list(range(a, b)),\n",
    "    end_token = b,\n",
    "    max_path_length = 3,\n",
    ")\n",
    "\n",
    "scores = score_paths(all_paths, graph.adjacency_matrix)\n",
    "\n",
    "out = sorted(zip(scores, all_paths), key=lambda z: z[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 34.3671875\n",
      "path:\n",
      "    embed:  Dallas\n",
      "     a collection of strings that make up a Dallas news station callsign\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 32.28662109375\n",
      "path:\n",
      "    embed:  state\n",
      "    the word \\\\\\\"state\\\\\\\" or phrases that include \\\\\\\"state\\\\\\\"\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 31.83984375\n",
      "path:\n",
      "    embed:  Dallas\n",
      "     references to geographic locations, especially in addresses\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 29.12492847442627\n",
      "path:\n",
      "    embed: <bos>\n",
      "     words related to a variety of coding languages, science, or other languages\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 26.622802734375\n",
      "path:\n",
      "    embed: <bos>\n",
      "     topics that are academic in nature, including math, linguistics/anthropology, religion, zoology, history or politics\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 24.70166015625\n",
      "path:\n",
      "    embed:  capital\n",
      "    the word \\\\\\\"capital\\\\\\\" and sometimes letters\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 24.265625\n",
      "path:\n",
      "    embed:  Dallas\n",
      "     references to geographic locations, especially in addresses\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n",
      "score: 22.625747680664062\n",
      "path:\n",
      "    embed: <bos>\n",
      "    a variety of numeric and statistical data, including p-values, BMI, ages, and measurements\n",
      "    logit:  Austin (0.4453125)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k = 8\n",
    "for score, path in out[:top_k]:\n",
    "    print(f'score: {score}\\npath:')\n",
    "    for node in path:\n",
    "        print(f'    ' + matrix_idx_to_explanation(node))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-circuit-tracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
